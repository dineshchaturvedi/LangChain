{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c57da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_PROJECT: GenAIAPPWithOPENAI\n"
     ]
    }
   ],
   "source": [
    "### Load the keys from the environment variable in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Langsmith tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" \n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "# Add a print statement to confirm its value right before LLM invocation\n",
    "print(f\"LANGCHAIN_TRACING_V2: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LANGCHAIN_PROJECT: {os.getenv('LANGCHAIN_PROJECT')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ab4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# print(llm) # Prints some object that I cannot make sense right now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap # helps you wrap text\n",
    "\n",
    "query = \"What is Generative API\"\n",
    "\n",
    "result = llm.invoke(query) # invoking GenAI to get my results from Python\n",
    "# result returned a AIObject\n",
    "wrappend_text = textwrap.fill(result.content, width=50) # I have to convert AIObject to String, using.Content\n",
    "# wrapping content for better readability\n",
    "print(wrappend_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1084bf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Please provide me answers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Please provide me answers based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd93bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|llm\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "wrappend_text =textwrap.fill(response.content,width=50)\n",
    "print(wrappend_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7aa6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a suite of development tools and\n",
      "services created by the team behind LangChain,\n",
      "aimed at facilitating the process of building\n",
      "language model applications. Launched in mid-2023,\n",
      "Langsmith enables developers to efficiently\n",
      "prototype, evaluate, and monitor applications that\n",
      "leverage large language models (LLMs).  The\n",
      "toolkit is designed to streamline the development\n",
      "workflow by providing features such as:  1.\n",
      "**Langchain Integration**: Seamless integration\n",
      "with LangChain, allowing developers to directly\n",
      "interface with and utilize its capabilities for a\n",
      "more cohesive development experience.  2.\n",
      "**Prototyping Tools**: Facilities for rapidly\n",
      "creating and iterating on language model-based\n",
      "applications, helping developers to quickly test\n",
      "hypotheses and refine their applications.  3.\n",
      "**Evaluation Mechanisms**: Tools that allow\n",
      "developers to assess the performance and accuracy\n",
      "of their applications through various metrics,\n",
      "ensuring that they meet desired standards and\n",
      "function as intended.  4. **Monitoring and\n",
      "Logging**: Features that enable continuous\n",
      "monitoring of applications in production,\n",
      "providing insights into their operation and\n",
      "helping identify areas for improvement or\n",
      "troubleshooting issues.  5. **Performance\n",
      "Improvement**: Utilities to optimize the\n",
      "deployment of language model applications,\n",
      "focusing on efficiency and effectiveness in real-\n",
      "world scenarios.  Langsmith's combination of\n",
      "LangChain integration, prototyping, evaluation,\n",
      "and monitoring capabilities makes it a\n",
      "comprehensive toolkit for developers looking to\n",
      "harness the power of large language models in\n",
      "their projects.\n"
     ]
    }
   ],
   "source": [
    "### String Output Parser, to display the messages in more readable format\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "outputparser = StrOutputParser()\n",
    "chain=prompt|llm|outputparser\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "## By using the StrOutputParser, you will only recieve contents and not the entire message\n",
    "# print(response)\n",
    "wrappend_text =textwrap.fill(response,width=50)\n",
    "### note since here we are getting only content (a string type and not AIMessage type) \n",
    "### in response variable, I do not have to do response.content\n",
    "print(wrappend_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
