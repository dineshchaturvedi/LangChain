{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e231941",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the keys from the environment variable in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f8b167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000015CC2971010>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000015CC2971BE0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "# model is an instance of ChatGroq. This is your Large Language Model (LLM) component. \n",
    "# When model is \"invoked\" (either directly or as part of a chain), it takes an input \n",
    "# (like your message list) and generates a response, which is typically \n",
    "# a BaseMessage object (or a list of them).\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5b2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "message=[\n",
    "    SystemMessage(content=\"Translate this message from English to French\"),\n",
    "    HumanMessage(content=\"Hello how are you?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef233a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, comment vas-tu ? \\n\\n(This is the most common and informal way to say it)\\n\\n\\nYou can also say:\\n\\n* **Bonjour, comment allez-vous ?** (More formal)\\n* **Salut, comment Ã§a va ?** (Very informal)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# parser: parser is an instance of StrOutputParser. This is an output parser component.\n",
    "# Its job is to take the raw output from the LLM \n",
    "# (which might be a complex object containing message type, content, etc.)\n",
    "# and transform it into a more usable format, in this case, a simple string.\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721af0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour, comment allez-vous\\xa0? \\n\\n\\nYou can also use:\\n\\n* **Salut, Ã§a va\\xa0?** (Informal)\\n* **Comment vas-tu\\xa0?** (Informal, singular) \\n\\n\\nLet me know if you have any other phrases you'd like translated!\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LangChain Expression Language (LCEL) is a syntax within the LangChain framework that allows \n",
    "developers to easily define and combine different components into complex workflows, particularly \n",
    "for building applications powered by large language models (LLMs).\n",
    "'''\n",
    "\n",
    "\n",
    "# Chain of components\n",
    "# the line chain = model | parser is a key part of the LangChain Expression Language (LCEL) and \n",
    "# represents a chaining mechanism that connects different components together.\n",
    "# | (Pipe Operator): In LCEL, the pipe operator | is used to \"pipe\" the output of the component \n",
    "# on its left to the input of the component on its right. It signifies a sequential execution flow.\n",
    "chain=model|parser\n",
    "chain.invoke(message)\n",
    "\n",
    "# In this method instead of invoking model.invoke(message) getting output in result\n",
    "# and then passing the result to parser.invoke(result)\n",
    "#  we are using chaining to get the desired result and pipe passes the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0fd0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour ! ðŸ˜Š  \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Templates\n",
    "# ChatPromptTemplate: This class is specifically designed to create prompts for chat models \n",
    "# (like the ChatGroq model we are using). Chat models typically take a list of messages,\n",
    "#  each with a role (e.g., \"system\", \"human\", \"ai\"), rather than a single string. \n",
    "# ChatPromptTemplate helps you construct these lists of messages in a structured and reusable way.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# This defines a simple string variable generic_template.\n",
    "# {language} is a placeholder within the string. It signifies that this part of the template will be \n",
    "# filled in dynamically when the prompt is actually used.\n",
    "\n",
    "generic_template =\"Translate Message into the {language}:\"\n",
    "\n",
    "# It creates an instance of ChatPromptTemplate. Let's look at the list passed to its constructor: \n",
    "# [(\"system\",generic_template),(\"user\"),\"{text}\"]. This list defines the structure of your chat prompt, \n",
    "# which will eventually be sent to the LLM.\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [(\"system\",generic_template),(\"user\"),\"{text}\"]\n",
    ")\n",
    "\n",
    "#  Below line creates a structured prompt, does not involve the model yet\n",
    "result = prompt.invoke({\"language\":\"french\",\"text\":\"hello\"})\n",
    "# this lines only defines the sequence of chain, but does not execute it\n",
    "chain = prompt|model|parser\n",
    "#  This is where invocation is happening, notice, I cannot use the result and have to pass\n",
    "#  parameters again because only runnable type can be used in a chain.\n",
    "chain.invoke({\"language\":\"french\",\"text\":\"hello\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
