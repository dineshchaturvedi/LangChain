{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac522875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C044337250>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C044337C50>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os                          # for operating system work\n",
    "from dotenv import load_dotenv     # to load environment variables \n",
    "from langchain_groq import ChatGroq # for the chat module\n",
    "\n",
    "load_dotenv() # loading all the environment variables\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\") # get the groq api key\n",
    "\n",
    "# Create Model from Groq, using the API Key and Model name\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca34e1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Dinesh, it's nice to meet you! \\n\\nThat's great you're an Associate Director at Accenture. What kind of work do you do there?  \\n\\nI'm eager to learn more about your role and experience.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 25, 'total_tokens': 80, 'completion_time': 0.1, 'prompt_time': 0.002310808, 'queue_time': 0.11594836, 'total_time': 0.102310808}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--d3c569e4-144d-4095-92d9-642faebfee51-0', usage_metadata={'input_tokens': 25, 'output_tokens': 55, 'total_tokens': 80})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi, I am Dinesh, I work at Accenture as an Associate Director\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c5adff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Dinesh, and you are an Associate Director at Accenture.  \\n\\nIs there anything else you'd like to tell me about your work or anything specific you'd like to discuss? üòä  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 97, 'total_tokens': 143, 'completion_time': 0.083636364, 'prompt_time': 0.005231385, 'queue_time': 0.091177177, 'total_time': 0.088867749}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--6cec5ddb-8aa0-4542-832d-b88c9d4e0656-0', usage_metadata={'input_tokens': 97, 'output_tokens': 46, 'total_tokens': 143})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, I am Dinesh, I work at Accenture as an Associate Director\"),\n",
    "        AIMessage(content=\"Hi Dinesh, it's nice to meet you! \\n\\nThat's great you're an Associate Director at Accenture. What kind of work do you do there?  \\n\\nI'm eager to learn more about your role and experience.\\n\"),\n",
    "        HumanMessage(content=\"Hey whats my name and what do I do\")\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80cdf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Message History\n",
    "#This imports the ChatMessageHistory class, which is a \n",
    "# concrete implementation of BaseChatMessageHistory. It's used to store chat messages.\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# This is a crucial class that wraps a runnable (like our ChatGroq model) and \n",
    "# imbues it with the ability to manage conversation history.\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#This line initializes an empty dictionary named store\n",
    "store={}\n",
    "#This defines a function named get_session_history that takes a sessionid (a string) as \n",
    "# input and is type-hinted to return an object conforming to BaseChatMessageHistory.\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "#This is where the magic happens for incorporating message history.\n",
    "#RunnableWithMessageHistory is instantiated with two arguments:\n",
    "# model: This is the ChatGroq model instance we created earlier. \n",
    "# This is the \"runnable\" that will be enhanced with history capabilities.\n",
    "# later when the invoke method is called on with_message_history, it will use the \n",
    "# stored model and get_session_history to call get_session_history\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e06f21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dictionary defines the configuration for the invocation.\n",
    "# \"configurable\": This key is used to pass configuration options to \n",
    "# runnables that support them.\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ba6af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line invokes the with_message_history runnable.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config #This passes the config dictionary we defined, which includes the session_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b33d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Dinesh, it's great to meet you!  Accenture is a huge company, so I'm curious - what area of work do you specialize in?  \\n\\nBeing an Associate Director is a leadership position, so what are some of the biggest challenges and rewards you've found in that role?\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80eb2032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to remember it! üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 14, 'total_tokens': 53, 'completion_time': 0.070909091, 'prompt_time': 0.002788692, 'queue_time': 0.18181709399999998, 'total_time': 0.073697783}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--20ccc02c-53b6-4d50-827d-8c0183d40b87-0', usage_metadata={'input_tokens': 14, 'output_tokens': 39, 'total_tokens': 53})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config, it should not remember\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config1 #This passes the config dictionary we defined, which includes the session_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9b7c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "#ChatPromptTemplate.from_messages is a class method (a method called directly on the class, \n",
    "# not an instance) that allows you to construct a ChatPromptTemplate from a list of \"message templates.\n",
    "# \" Each element in the list represents a part of the conversation, specifying who is speaking (role)\n",
    "#  and what their message is (content).It's designed specifically for chat models (like your ChatGroq model)\n",
    "#  because chat models understand conversational turns (system messages, human messages, AI messages, etc.)\n",
    "prompt=ChatPromptTemplate.from_messages (\n",
    "    [\n",
    "        (\"system\",\"You are a help assitant. Answer all the questions to the best of your abaility\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1faa7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Dinesh!\\n\\nIt's nice to meet you. I'm happy to help with any questions you have.  Just ask away! üòä \\n\\nWhat can I do for you today? \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 32, 'total_tokens': 78, 'completion_time': 0.083636364, 'prompt_time': 0.002301304, 'queue_time': 0.090613052, 'total_time': 0.085937668}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--005a55ff-7da2-47bf-9bbb-922078c1acbf-0', usage_metadata={'input_tokens': 32, 'output_tokens': 46, 'total_tokens': 78})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"My name is Dinesh\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42eb0f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Dinesh, it's nice to meet you!\\n\\nIs there anything I can help you with today? üòä\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a new config called chat3\n",
    "config1={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is Dinesh\")],\n",
    "    config=config1 #This passes the config dictionary we defined, which includes the session_id\n",
    ")\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more complexity, change the prompt to take a variable language \n",
    "prompt=ChatPromptTemplate.from_messages (\n",
    "    [\n",
    "        (\"system\",\"You are a help assitant. Answer all the questions to the best of your abaility in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "# I have to rerun the prompt here because the defination is stored and its not a link\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b96ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á, Dinesh! üòä  ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§  ‡§Ü‡§™‡§ï‡•á ‡§ï‡•ã‡§à ‡§∏‡§µ‡§æ‡§≤ ‡§π‡•à?  \\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ChatPromptTemplate is designed to take a dictionary as its input if it contains multiple variables.\n",
    "# It looks for keys in this input dictionary that match the placeholders\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"My name is Dinesh\")],\n",
    "     \"language\":\"hindi\"\n",
    "     }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d3de097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c0369b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡§ø‡§®‡•á‡§∂! üòä \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•ã‡§ó‡•Ä‡•§ ‡§Ü‡§™‡§ï‡•á ‡§ï‡•ã‡§à ‡§∏‡§µ‡§æ‡§≤ ‡§π‡•à‡§Ç? \\n\\n‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•Å‡§õ ‡§≠‡•Ä ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•Ç‡§Å‡§ó‡§æ ‡§Ü‡§™‡§ï‡•ã ‡§∏‡§π‡•Ä ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡§æ‡•§ \\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a new config called chat4\n",
    "config1={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response4=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"My name is Dinesh\")],\n",
    "     \"language\":\"Hindi\"},\n",
    "    config=config1\n",
    ")\n",
    "response4.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed86c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm Diensh\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like Kheer Mohan Bengali Sweet', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "#trim_messages: This is a crucial new component. It's a runnable factory \n",
    "# (meaning, it's a function that returns a Runnable instance) designed to intelligently shorten\n",
    "#  a list of chat messages based on a token limit. This is vital for managing LLM context \n",
    "# windows and controlling costs\n",
    "\n",
    "#trim_messages: This is a crucial new component. It's a runnable factory (meaning, \n",
    "# it's a function that returns a Runnable instance) designed to intelligently shorten a list of chat \n",
    "# messages based on a token limit. This is vital for managing LLM context windows and controlling costs\n",
    "trimmer=trim_messages(\n",
    "    max_tokens = 60, #This sets the maximum number of tokens the trimmed message history should occupy. \n",
    "    strategy = \"last\", #This dictates how messages are removed if max_tokens is exceeded.\n",
    "    token_counter=model, #This is very important. trim_messages needs a way to count tokens accurately. \n",
    "                         #By passing your model (the ChatGroq instance), you're providing a \n",
    "                         # token counter that understands how Groq counts tokens, ensuring the max_tokens \n",
    "                         # limit is respected precisely for your chosen LLM.\n",
    "    include_system=True, #the system message (if present) will always be included\n",
    "    allow_partial=False, #If True, it might allow a message to be partially included \n",
    "                         #if splitting it fits the token limit.\n",
    "    start_on=\"human\" #means it will ensure that the first message\n",
    "                     # kept in the history, after system messages, is a HumanMessag\n",
    "\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm Diensh\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like Kheer Mohan Bengali Sweet\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n",
    "\n",
    "#initially max_tokens were set to 70 and it returned all value, when changed to 45 it trimmed messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You said you like Kheer Mohan! üòä  \\n\\nIs there anything else you'd like to chat about?  Perhaps other types of sweets, or something completely different?  \\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itemgetter: This is a utility function from Python's operator module.\n",
    "# It's often used in LangChain's LCEL for extracting specific items (keys) from an input dictionary.\n",
    "from operator import itemgetter\n",
    "#This is another powerful LCEL component. It literally \"passes through\" its input as its output.\n",
    "# It's often used with .assign() to inject or modify keys in the input dictionary without changing \n",
    "# the original input.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    |prompt\n",
    "    |model\n",
    ")\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"Which Bengali Sweet, I like?\")],\n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
