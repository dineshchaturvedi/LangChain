{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac522875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                          # for operating system work\n",
    "from dotenv import load_dotenv     # to load environment variables \n",
    "from langchain_groq import ChatGroq # for the chat module of the Groq\n",
    "\n",
    "load_dotenv() # loading all the environment variables from the .env file\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\") # get the groq api key\n",
    "\n",
    "# Create Model from Groq, using the API Key and Model name\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi, I am Dinesh, I work at Accenture as an Associate Director\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5adff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Dinesh, and you are an Associate Director at Accenture.  \\n\\nIs there anything else you'd like to tell me about your work there? 😊  I'm here to listen! \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 97, 'total_tokens': 143, 'completion_time': 0.083636364, 'prompt_time': 0.00266732, 'queue_time': 0.070719377, 'total_time': 0.086303684}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--486ef362-c0c5-4569-b8e0-3721d312c576-0', usage_metadata={'input_tokens': 97, 'output_tokens': 46, 'total_tokens': 143})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, I am Dinesh, I work at Accenture as an Associate Director\"),\n",
    "        AIMessage(content=\"Hi Dinesh, it's nice to meet you! \\n\\nThat's great you're an Associate Director at Accenture. What kind of work do you do there?  \\n\\nI'm eager to learn more about your role and experience.\\n\"),\n",
    "        HumanMessage(content=\"Hey whats my name and what do I do\")\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fc0d7",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cdf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Message History\n",
    "#This imports the ChatMessageHistory class, which is a \n",
    "# concrete implementation of BaseChatMessageHistory. It's used to store chat messages.\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# This is a crucial class that wraps a runnable (like our ChatGroq model) and \n",
    "# imbues it with the ability to manage conversation history.\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#This line initializes an empty dictionary named store\n",
    "store={}\n",
    "#This defines a function named get_session_history that takes a sessionid (a string) as \n",
    "# input and is type-hinted to return an object conforming to BaseChatMessageHistory.\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "#This is where the magic happens for incorporating message history.\n",
    "#RunnableWithMessageHistory is instantiated with two arguments:\n",
    "# model: This is the ChatGroq model instance we created earlier. \n",
    "# This is the \"runnable\" that will be enhanced with history capabilities.\n",
    "# later when the invoke method is called on with_message_history, it will use the \n",
    "# stored model and get_session_history to call get_session_history\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06f21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dictionary defines the configuration for the invocation.\n",
    "# \"configurable\": This key is used to pass configuration options to \n",
    "# runnables that support them.\n",
    "\n",
    "config1={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba6af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line invokes the with_message_history runnable.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is dinesh and I am an associate director at Accenture\")],\n",
    "    config=config1 #This passes the config dictionary we defined, which includes the session_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b33d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Dinesh! It's nice to meet you. Being an associate director at Accenture is quite impressive. \\n\\nIs there anything I can help you with today? Perhaps you have a question about Accenture, need help brainstorming ideas, or just want to chat?  I'm here to assist in any way I can. \\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80eb2032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to know! 😊\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 14, 'total_tokens': 52, 'completion_time': 0.069090909, 'prompt_time': 0.001292109, 'queue_time': 0.070070933, 'total_time': 0.070383018}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--beea92e7-15b2-4bcd-a1a8-5d06ef2595fb-0', usage_metadata={'input_tokens': 14, 'output_tokens': 38, 'total_tokens': 52})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config, it should not remember\n",
    "config2={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config2 #This passes the config dictionary we defined, which includes the session_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e160cb7",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n",
    "\n",
    "### Runnable protocol\n",
    "ChatPromptTemplate is a core component of LangChain and is a full-fledged Runnable.\n",
    "\n",
    "This means it adheres to the Runnable protocol and has methods like .invoke(), .batch(), .stream(), and .transform(), just like other runnables you've seen (e.g., ChatGroq, RunnablePassthrough).\n",
    "\n",
    "### Using as Runnable \n",
    "We have already been using ChatPromptTemplate as a runnable in the code below. The pipe (|) operator from LangChain Expression Language (LCEL) only works between Runnable objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c95f95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b7c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "#ChatPromptTemplate.from_messages is a class method (a method called directly on the class, \n",
    "# not an instance) that allows you to construct a ChatPromptTemplate from a list of \"message templates.\n",
    "# \" Each element in the list represents a part of the conversation, specifying who is speaking (role)\n",
    "#  and what their message is (content).It's designed specifically for chat models (like your ChatGroq model)\n",
    "#  because chat models understand conversational turns (system messages, human messages, AI messages, etc.)\n",
    "prompt=ChatPromptTemplate.from_messages (\n",
    "    [\n",
    "        (\"system\",\"You are a help assitant. Answer all the questions to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "        # MessagesPlaceholder(variable_name=\"messages\") is a crucial component in the ChatPromptTemplate. \n",
    "        # Its primary purpose is to act as a placeholder for the entire chat history. \n",
    "        # Its basically the name of the variable which you can assign new messages\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1faa7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Pawan Deep Singh, it's nice to meet you! \\n\\nI understand you're a friend of Dinesh who lives in Bhopal, India. How can I help you today? \\n\\nDo you have any questions about Bhopal, India? Or perhaps you'd like me to help you find something specific?  \\n\\nI'm here to assist you in any way I can. 😊 \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 46, 'total_tokens': 133, 'completion_time': 0.158181818, 'prompt_time': 0.001630099, 'queue_time': 0.069961276, 'total_time': 0.159811917}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--7b557c92-275d-4dc8-a655-0388255001be-0', usage_metadata={'input_tokens': 46, 'output_tokens': 87, 'total_tokens': 133})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"My name is Pawan Deep Singh and I am the friend of Dinesh living in Bhopal India\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb0f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Dinesh! \\n\\nIt's nice to meet you.  I'm ready to help with any questions you have. \\n\\nWhat can I do for you today? 😊  \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a new config called chat3\n",
    "config3={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "# We passed chain instead of the model here.\n",
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is Dinesh\")],\n",
    "    config=config3 #This passes the config dictionary we defined, which includes the session_id\n",
    ")\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f4c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more complexity, change the prompt to take a variable language \n",
    "prompt=ChatPromptTemplate.from_messages (\n",
    "    [\n",
    "        (\"system\",\"You are a help assitant. Answer all the questions to the best of your abaility in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "# I have to rerun the prompt here because the defination is stored and its not a link\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9b96ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते दिनेश! \\n\\nमैं आपकी सहायता करने के लिए यहाँ हूँ। आप मुझसे कुछ भी पूछ सकते हैं, मैं अपनी क्षमताओं के अनुसार उत्तर देने की पूरी कोशिश करूँगा। \\n\\nआपको क्या जानना है? 😊 \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ChatPromptTemplate is designed to take a dictionary as its input if it contains multiple variables.\n",
    "# It looks for keys in this input dictionary that match the placeholders\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"My name is Dinesh\")],\n",
    "     \"language\":\"hindi\"\n",
    "     }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d3de097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c0369b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते Dinesh! \\n\\nमुझे आपकी मदद करने में खुशी हो रही है। आप क्या जानना चाहते हैं? \\n\\nक्या कोई विशेष प्रश्न है या मैं कुछ भी और कैसे मदद कर सकता हूँ? \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a new config called chat4\n",
    "config4={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response4=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"My name is Dinesh\")],\n",
    "     \"language\":\"Hindi\"},\n",
    "    config=config4\n",
    ")\n",
    "response4.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed86c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "#trim_messages: This is a crucial new component. It's a runnable factory \n",
    "# (meaning, it's a function that returns a Runnable instance) designed to intelligently shorten\n",
    "#  a list of chat messages based on a token limit. This is vital for managing LLM context \n",
    "# windows and controlling costs\n",
    "\n",
    "#trim_messages: This is a crucial new component. It's a runnable factory (meaning, \n",
    "# it's a function that returns a Runnable instance) designed to intelligently shorten a list of chat \n",
    "# messages based on a token limit. This is vital for managing LLM context windows and controlling costs\n",
    "trimmer=trim_messages(\n",
    "    max_tokens = 60, #This sets the maximum number of tokens the trimmed message history should occupy. \n",
    "    strategy = \"last\", #This dictates how messages are removed if max_tokens is exceeded.\n",
    "    token_counter=model, #This is very important. trim_messages needs a way to count tokens accurately. \n",
    "                         #By passing your model (the ChatGroq instance), you're providing a \n",
    "                         # token counter that understands how Groq counts tokens, ensuring the max_tokens \n",
    "                         # limit is respected precisely for your chosen LLM.\n",
    "    include_system=True, #the system message (if present) will always be included\n",
    "    allow_partial=False, #If True, it might allow a message to be partially included \n",
    "                         #if splitting it fits the token limit.\n",
    "    start_on=\"human\" #means it will ensure that the first message\n",
    "                     # kept in the history, after system messages, is a HumanMessag\n",
    "\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm Diensh\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like Kheer Mohan Bengali Sweet\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n",
    "\n",
    "#initially max_tokens were set to 70 and it returned all value, when changed to 45 it trimmed messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b0567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You said you like Kheer Mohan!  🍰 \\n\\nIs there anything else you'd like to talk about?  Maybe other delicious Bengali sweets?  😋\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itemgetter: This is a utility function from Python's operator module.\n",
    "# It's often used in LangChain's LCEL for extracting specific items (keys) from an input dictionary.\n",
    "    # In simple language it extracts the messages you pass as a list\n",
    "from operator import itemgetter\n",
    "#This is another powerful LCEL component. It literally \"passes through\" its input as its output.\n",
    "# It's often used with .assign() to inject or modify keys in the input dictionary without changing \n",
    "# the original input.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    |prompt\n",
    "    |model\n",
    ")\n",
    "# The defination of chain is given above, but actual invocation happens after below code is invoked.\n",
    "# messages present in the dictionary is passed to the itemgetter which converts it to the list\n",
    "# The list is then passed to the trimmer.\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"Which Bengali Sweet, I like?\")],\n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77ca16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
